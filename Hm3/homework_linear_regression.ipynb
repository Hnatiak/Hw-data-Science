{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6294900f-c961-4f92-9b2e-82295b8efa74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: Cost 48.70712873683291\n",
      "Iteration 100: Cost 0.6009339361253098\n",
      "Iteration 200: Cost 0.500666346041834\n",
      "Iteration 300: Cost 0.4526432359651629\n",
      "Iteration 400: Cost 0.4247779957164337\n",
      "Iteration 500: Cost 0.40750186633800756\n",
      "Iteration 600: Cost 0.39598520903126966\n",
      "Iteration 700: Cost 0.387756874857662\n",
      "Iteration 800: Cost 0.3815264917468388\n",
      "Iteration 900: Cost 0.3765993580975837\n",
      "Parameters from Gradient Descent: [3.25409614 3.24235198 2.15102683 1.59783892]\n",
      "Parameters from Normal Equation: [3.91498956 3.05294884 1.962303   1.34302962]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Функція гіпотези лінійної регресії\n",
    "def hypothesis(X, w):\n",
    "    return np.dot(X, w)\n",
    "\n",
    "# Функція обчислення функції втрат\n",
    "def compute_cost(X, y, w):\n",
    "    m = len(y)\n",
    "    h = hypothesis(X, w)\n",
    "    return (1/(2*m)) * np.dot((h - y).T, (h - y))\n",
    "\n",
    "# Один крок градієнтного спуску\n",
    "def gradient_descent_step(X, y, w, learning_rate):\n",
    "    m = len(y)\n",
    "    h = hypothesis(X, w)\n",
    "    gradient = (1/m) * np.dot(X.T, (h - y))\n",
    "    w = w - learning_rate * gradient\n",
    "    return w\n",
    "\n",
    "# Знайдення параметрів за допомогою градієнтного спуску\n",
    "def train_linear_regression(X, y, learning_rate=0.01, num_iterations=1000):\n",
    "    m, n = X.shape\n",
    "    w = np.zeros(n)\n",
    "    for i in range(num_iterations):\n",
    "        w = gradient_descent_step(X, y, w, learning_rate)\n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(X, y, w)\n",
    "            print(f\"Iteration {i}: Cost {cost}\")\n",
    "    return w\n",
    "\n",
    "# Знайдення параметрів за допомогою аналітичного рішення\n",
    "def normal_equation(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "# Генеруємо дані для прикладу\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 3)  # Площа, кількість ванних кімнат, кількість спалень\n",
    "y = 4 + 3 * X[:, 0] + 2 * X[:, 1] + X[:, 2] + np.random.randn(100)\n",
    "\n",
    "# Додаємо стовпець одиниць для перехоплення\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Розділяємо дані на тренувальні та тестові набори\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_b, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Навчання за допомогою градієнтного спуску\n",
    "w_gd = train_linear_regression(X_train, y_train)\n",
    "\n",
    "# Навчання за допомогою аналітичного рішення\n",
    "w_ne = normal_equation(X_train, y_train)\n",
    "\n",
    "print(\"Parameters from Gradient Descent:\", w_gd)\n",
    "print(\"Parameters from Normal Equation:\", w_ne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acb7b01-c71d-480b-8a8b-453fbb521c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
